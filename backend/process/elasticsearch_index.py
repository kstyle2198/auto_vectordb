import numpy as np
from process.postgres import PostgresPipeline
from elasticsearch import Elasticsearch, helpers
from elasticsearch.helpers import BulkIndexError

# ì„¤ì • ë° ë¡œê±° ë¡œë“œ (ê¸°ì¡´ ì½”ë“œë¥¼ ë”°ë¦„)
from utils.config import get_config
from utils.setlogger import setup_logger
config = get_config()
logger = setup_logger(f"{__name__}", level=config.LOG_LEVEL)

class ElasticsearchIndexer:
    """
    Elasticsearch ì—°ê²°, ì¸ë±ìŠ¤ ê´€ë¦¬ ë° ë°ì´í„° ìƒ‰ì¸ ì‘ì—…ì„ ìº¡ìŠí™”í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """

    def __init__(self, es_url: str = "http://localhost:9200", index_name: str = "test_002"):
        """
        ElasticsearchIndexer í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  Elasticsearch ì—°ê²° ë° ì¸ë±ìŠ¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        self.INDEX_NAME = index_name
        self.es = Elasticsearch(es_url)
        self.pg_pipe = PostgresPipeline() # PostgreSQL íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ (ê°€ì •)
        self.mapping = {
            "mappings": {
                "properties": {
                    "id": { "type": "keyword" },
                    "page_content": { "type": "text" },
                    "filename": { "type": "keyword" },
                    "filepath": { "type": "keyword" },
                    "hashed_filename": { "type": "keyword" },
                    "hashed_filepath": { "type": "keyword" },
                    "hashed_page_content": { "type": "keyword" },
                    "page": { "type": "keyword" },
                    "lv1_cat": { "type": "keyword" },
                    "lv2_cat": { "type": "keyword" },
                    "lv3_cat": { "type": "keyword" },
                    "lv4_cat": { "type": "keyword" },
                    "embeddings": {
                        "type": "dense_vector",
                        "dims": 1024
                    },
                    "created_at": { "type": "date" },
                    "updated_at": { "type": "date" }
                }
            }
        }
        
        self._ensure_index_exists()

    def _ensure_index_exists(self):
        """
        Elasticsearch ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
        """
        if not self.es.indices.exists(index=self.INDEX_NAME):
            try:
                self.es.indices.create(index=self.INDEX_NAME, body=self.mapping)
                logger.info(f"Elasticsearch Index '{self.INDEX_NAME}' created successfully.")
            except Exception as e:
                logger.error(f"Error creating index '{self.INDEX_NAME}': {e}")
                # ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨ ì‹œ ì¶”ê°€ì ì¸ ì—ëŸ¬ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        else:
            logger.info(f"Index '{self.INDEX_NAME}' already exists.")


    @staticmethod
    def _convert_numpy_types(obj):
        """
        ì¬ê·€ì ìœ¼ë¡œ numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (ì •ì  ë©”ì„œë“œë¡œ ìœ ì§€)
        """
        if isinstance(obj, dict):
            return {k: ElasticsearchIndexer._convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [ElasticsearchIndexer._convert_numpy_types(x) for x in obj]
        elif isinstance(obj, np.ndarray):
            return obj.astype(float).tolist()
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        else:
            return obj

    @staticmethod
    def _parse_embedding_string(val):
        """
        PostgreSQLì—ì„œ ì €ì¥ëœ embedding ë¬¸ìì—´ì„ float ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì •ì  ë©”ì„œë“œë¡œ ìœ ì§€)
        """
        if not val:
            return []
        
        # ì´ë¯¸ listë¼ë©´ ê·¸ëƒ¥ ì‚¬ìš©
        if isinstance(val, list):
            try:
                 return [float(x) for x in val]
            except ValueError:
                 return [] # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        # ë¬¸ìì—´ ì²˜ë¦¬: '{-0.07,...}' â†’ [-0.07,...]
        if isinstance(val, str):
            val = val.strip("{}")
            try:
                return [float(x) for x in val.split(",") if x.strip()]
            except ValueError:
                return []

        # numpy array ì²˜ë¦¬
        if isinstance(val, np.ndarray):
            return val.astype(float).tolist()

        return []

    def _generate_actions(self, rows):
        """
        Elasticsearchì˜ helpers.bulk()ë¥¼ ìœ„í•œ ì•¡ì…˜ì„ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„°ì…ë‹ˆë‹¤.
        """
        for r in rows:
            # PostgreSQL ì¿¼ë¦¬ ê²°ê³¼ì˜ ì¸ë±ìŠ¤ì— ë§¤í•‘ë˜ëŠ” í•„ë“œ
            doc = {
                "id": r[0],
                "page_content": r[1],
                "filename": r[2],
                "filepath": r[3],
                "hashed_filename": r[4],
                "hashed_filepath": r[5],
                "hashed_page_content": r[6],
                "page": r[7],
                "lv1_cat": r[8],
                "lv2_cat": r[9],
                "lv3_cat": r[10],
                "lv4_cat": r[11],
                "embeddings": self._parse_embedding_string(r[12]),
                "created_at": r[13],
                "updated_at": r[14],
            }
            # numpy íƒ€ì… ì•ˆì „ ë³€í™˜
            doc = self._convert_numpy_types(doc)

            yield {
                "_index": self.INDEX_NAME,
                "_id": r[0], 
                "_source": doc
            }

    def index_documents_by_hashed_filepath(self, table_name: str, hashed_filepath: str):
        """
        ì£¼ì–´ì§„ hashed_filepathì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ PostgreSQLì—ì„œ ê°€ì ¸ì™€ Elasticsearchì— ìƒ‰ì¸í•©ë‹ˆë‹¤.
        """
        logger.info(f"Fetching rows for hashed_filepath: {hashed_filepath}")
        
        # PostgreSQLPipelineì„ í†µí•´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (PostgresPipeline ë©”ì„œë“œ ê°€ì •)
        rows = self.pg_pipe.get_row_by_hashed_filepath(table_name=table_name, hashed_filepath=hashed_filepath)

        if not rows:
            logger.warning(f"No rows found for hashed_filepath: {hashed_filepath} in table: {table_name}")
            return

        try:
            # helpers.bulkë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¼ê´„ ìƒ‰ì¸
            successes, errors = helpers.bulk(self.es, self._generate_actions(rows), raise_on_error=False)
            
            if errors:
                 logger.warning(f"{len(errors)} document(s) failed to index. First error: {errors[0]}")

            logger.info(f"Successfully indexed {successes} out of {len(rows)} documents to Elasticsearch.")
            
        except BulkIndexError as e:
            # raise_on_error=Falseë¡œ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ ì´ ì˜ˆì™¸ëŠ” ë°œìƒí•˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ,
            # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
            logger.error(f"A general BulkIndexError occurred: {e.errors}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during indexing: {e}")

    def search_documents_by_hashed_filepath(self, hashed_filepath: str):
        """
        Elasticsearchì—ì„œ ì£¼ì–´ì§„ hashed_filepathì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ì´ëŠ” í•˜ë‚˜ì˜ íŒŒì¼(ê²½ë¡œ)ì— ì†í•œ ëª¨ë“  í˜ì´ì§€/ì²­í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
        """
        
        # 1. Elasticsearch ì¿¼ë¦¬ ì •ì˜
        # 'keyword' íƒ€ì… í•„ë“œì— ëŒ€í•´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê°’ì„ ì°¾ê¸° ìœ„í•´ 'term' ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        search_body = {
            "query": {
                "term": {
                    "hashed_filepath": hashed_filepath # hashed_filepath í•„ë“œì— ëŒ€í•œ ì •í™•í•œ ì¼ì¹˜ ê²€ìƒ‰
                }
            },
            "size": 10000  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (í•„ìš”ì— ë”°ë¼ ì¡°ì •)
        }
        
        logger.info(f"Searching documents for hashed_filepath: {hashed_filepath}")
        
        try:
            # 2. ê²€ìƒ‰ ì‹¤í–‰
            res = self.es.search(index=self.INDEX_NAME, body=search_body)
            
            # 3. ê²°ê³¼ íŒŒì‹± ë° ë°˜í™˜
            hits = res['hits']['hits']
            documents = [hit['_source'] for hit in hits]
            
            logger.info(f"Found {len(documents)} documents for hashed_filepath: {hashed_filepath}")
            
            return documents
            
        except self.es.exceptions.NotFoundError:
            # ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
            logger.error(f"Index '{self.INDEX_NAME}' not found.")
            return []
        except Exception as e:
            logger.error(f"Error searching documents by hashed_filepath: {e}")
            return []


# ğŸ‘‡ï¸ ìš”ì²­í•˜ì‹  ê²€ìƒ‰ ë©”ì„œë“œ ì¶”ê°€
    def search_documents(self, query_text: str = None, query_embedding: list = None, size: int = 10, min_score: float = 0.5):
        """
        Elasticsearchì—ì„œ í…ìŠ¤íŠ¸ ë˜ëŠ” ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        í…ìŠ¤íŠ¸ ê²€ìƒ‰ (query_text)ê³¼ ë²¡í„° ê²€ìƒ‰ (query_embedding) ì¤‘ í•˜ë‚˜ ë˜ëŠ” ë‘˜ ë‹¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ë‘˜ ë‹¤ ì œê³µë˜ë©´ ë¶€ìŠ¤íŠ¸ ê°’ì´ ì ìš©ëœ Bool ì¿¼ë¦¬ (RRF)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        
        Args:
            query_text (str, optional): ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì–´. Defaults to None.
            query_embedding (list, optional): ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ 1024ì°¨ì› ì„ë² ë”© ë¦¬ìŠ¤íŠ¸. Defaults to None.
            size (int, optional): ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜. Defaults to 10.
            min_score (float, optional): ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’. Defaults to 0.5.
            
        Returns:
            list: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ (hit['_source'] + score) ë¦¬ìŠ¤íŠ¸.
        """
        if not query_text and not query_embedding:
            logger.warning("Either query_text or query_embedding must be provided.")
            return []

        search_body = {
            "size": size,
            "min_score": min_score,
            "query": {
                "bool": {
                    "should": [],
                    "minimum_should_match": 1 # 'should' ì ˆ ì¤‘ ìµœì†Œ í•˜ë‚˜ëŠ” ì¼ì¹˜í•´ì•¼ í•¨
                }
            },
            # Elasticsearch 8.x ì´ìƒì—ì„œ kNN ê²€ìƒ‰ì„ ìœ„í•œ kNN ì„¹ì…˜ ì¶”ê°€ (Elasticsearch ë²„ì „ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
            "knn": []
        }
        
        # 1. ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¿¼ë¦¬ (Query Text)
        if query_text:
            # "page_content" í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ëŠ” match ì¿¼ë¦¬ ì¶”ê°€
            search_body["query"]["bool"]["should"].append({
                "match": {
                    "page_content": {
                        "query": query_text,
                        "boost": 1.0 # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë¶€ìŠ¤íŠ¸ ê°’
                    }
                }
            })
            logger.info(f"Text search enabled for: {query_text}")
        
        # 2. ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ (Query Embedding)
        if query_embedding:
            if len(query_embedding) != 1024:
                logger.error(f"Embedding must be 1024 dimensions, got {len(query_embedding)}")
                return []
            
            # kNN ì„¹ì…˜ì— dense_vector ê²€ìƒ‰ ì¶”ê°€
            # ì°¸ê³ : Elasticsearch 8.x ë²„ì „ì—ì„œëŠ” search APIì˜ 'knn' íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜
            # 7.x ë²„ì „ì—ì„œëŠ” 'script_score' ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” 8.xì˜ 'knn' íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤.
            search_body["knn"].append({
                "field": "embeddings",
                "query_vector": query_embedding,
                "k": size, # k: ì´ì›ƒ ìˆ˜
                "num_candidates": max(size * 10, 50), # ê²€ìƒ‰í•  í›„ë³´ ìˆ˜ (ì„±ëŠ¥/ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)
                "boost": 0.8 # ë²¡í„° ê²€ìƒ‰ ë¶€ìŠ¤íŠ¸ ê°’ (í…ìŠ¤íŠ¸ ê²€ìƒ‰ë³´ë‹¤ ì•½ê°„ ë‚®ê²Œ ì„¤ì •)
            })
            
            # kNNì„ ì‚¬ìš©í•  ê²½ìš°, ìµœì†Œ ì ìˆ˜ ëŒ€ì‹  í•„í„°ë§ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œë¥¼ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì´ ì˜ˆì‹œì—ì„œëŠ” min_scoreë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
            
            logger.info("Vector search enabled.")
            
        try:
            # 3. ê²€ìƒ‰ ì‹¤í–‰
            # Elasticsearch 8.xì—ì„œëŠ” kNNê³¼ ì¿¼ë¦¬ë¥¼ ì¡°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # 'knn' íŒŒë¼ë¯¸í„°ê°€ ë¹„ì–´ ìˆì§€ ì•Šìœ¼ë©´ 'search_body'ì—ì„œ 'knn'ì„ ì œê±°í•˜ê³  ë³„ë„ì˜ 'knn' ì¸ìˆ˜ë¡œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
            # í•˜ì§€ë§Œ 8.x í´ë¼ì´ì–¸íŠ¸ì˜ search ë©”ì„œë“œê°€ bodyì— knnì„ í—ˆìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ bodyì— í¬í•¨í•©ë‹ˆë‹¤.
            res = self.es.search(
                index=self.INDEX_NAME, 
                body=search_body
            )
            
            # 4. ê²°ê³¼ íŒŒì‹± ë° ë°˜í™˜
            hits = res['hits']['hits']
            
            # ê²°ê³¼ì— ì ìˆ˜ (Relevance Score)ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
            documents = [{'_score': hit['_score'], **hit['_source']} for hit in hits]
            
            logger.info(f"Found {len(documents)} documents.")
            
            return documents
            
        except Exception as e:
            logger.error(f"Error searching documents: {e}")
            return []
        
    def get_all_index_names(self):
        """
        Elasticsearch í´ëŸ¬ìŠ¤í„°ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ì¸ë±ìŠ¤ì˜ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        try:
            # indices.get_alias("*")ëŠ” ëª¨ë“  ì¸ë±ìŠ¤ì˜ ë³„ì¹­ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ë©°, 
            # ë”•ì…”ë„ˆë¦¬ì˜ í‚¤(key)ê°€ ì¸ë±ìŠ¤ ì´ë¦„ì…ë‹ˆë‹¤.
            indices_dict = self.es.indices.get_alias(index="*")
            index_names = list(indices_dict.keys())
            
            logger.info(f"Retrieved {len(index_names)} indices from Elasticsearch.")
            return indices_dict
            
        except Exception as e:
            logger.error(f"Error fetching all index names: {e}")
            return []

    def delete_index_by_name(self, index_name: str):
        """
        ì§€ì •ëœ ì´ë¦„ì˜ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        
        Args:
            index_name (str): ì‚­ì œí•  ì¸ë±ìŠ¤ì˜ ì´ë¦„
        
        Returns:
            bool: ì‚­ì œ ì„±ê³µ ì‹œ True, ì‹¤íŒ¨í•˜ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ False
        """
        if not index_name:
            logger.warning("No index name provided for deletion.")
            return False

        try:
            if self.es.indices.exists(index=index_name):
                self.es.indices.delete(index=index_name)
                logger.info(f"Index '{index_name}' has been deleted successfully.")
                return True
            else:
                logger.warning(f"Index '{index_name}' does not exist, skipping deletion.")
                return False
                
        except Exception as e:
            logger.error(f"Error deleting index '{index_name}': {e}")
            return False

if __name__ == "__main__":
    
    es = ElasticsearchIndexer()
    test_key = "5476ca42f4dd6e62009b59289f1c7f84"  # ì˜ˆì‹œ hashed_filepath
    # es.index_by_hashed_filepath(INDEX_NAME, test_key)
    # pass

    # ì¡°íšŒ ì˜ˆì‹œ ì‚¬ìš©
    doc = es.get_document_by_id(test_key)
    print(doc)
    pass